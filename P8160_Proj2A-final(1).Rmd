---
title: "P8160_Proj2A"
author: "Zhengkun Ou, Jianming Wang, Alice Zhou, Huizhong Peng"
date: "2025-03-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(numDeriv)
library(parallel)
library(ggplot2)
```

## Introduction

Sampling from complex probability distributions is a fundamental challenge in statistical computing, particularly when the target distribution exhibits skewness, heavy tails, or multiple modes. Traditional acceptance-rejection sampling provides a straightforward solution but can suffer from inefficiency when the proposal distribution poorly matches the target. To address these limitations, advanced techniques such as Adaptive Rejection Sampling (ARS) and Slice Sampling have been developed.

In this study, we implement and compare three sampling methods:

1. Basic Acceptance-Rejection Sampling 
2. Adaptive Rejection Sampling (ARS) 
3. Slice Sampling

Our goal is to evaluate their efficiency and accuracy when generating samples from a non-trivial target distribution that cannot be easily sampled using the inverse transform method. We assess each method based on acceptance rate, computational efficiency, and accuracy in approximating the target distribution. Additionally, we explore parallelization to improve computational performance.

By conducting this comparative analysis, we aim to provide insights into when advanced sampling methods offer practical advantages over basic rejection sampling, with implications for applications in Bayesian inference, Monte Carlo simulations, and statistical modeling.


### Target Distribution

Our target distribution is:
$$ f(x) = e^{0.4(x - 0.4)^2 - 0.08x^4} $$
```{r}
# target distribution
f <- function(x) {
  exp(0.4 * (x - 0.4)^2 - 0.08 * x^4)
}

# normalized
norm_const <- integrate(f, lower = -Inf, upper = Inf)$value
normalized_f <- function(x) {
  f(x) / norm_const
}

x_vals <- seq(-5, 5, length.out = 1000)
y_vals <- normalized_f(x_vals)

plot(x_vals, y_vals, type = "l", col = "red", lwd = 2,
     main = "Target Distribution",
     xlab = "x", ylab = "Density")
```

This distribution is non-trivial to sample. Inverse cumulative distribution function (CDF) method is not applicable here because the CDF is complex due to its bimodality and skewness, which makes computing the inverse CDF ($ F^{-1} $) inefficient and impractical.

Since the inverse CDF method is not feasible, we instead explore three alternative sampling methods to compare their efficiency:

1. Basic Acceptance-Rejection (AR) Sampling: Uses a proposal distribution and rejects samples based on the ratio of target to proposal density.

2. Adaptive Rejection Sampling (ARS): Constructs piecewise linear upper bounds of the log-density to increase efficiency.

3. Slice Sampling: Introduces an auxiliary variable to sample from horizontal "slices" of the density.


## Method

### Basic Acceptance-Rejection Algorithm (AR)

The **Acceptance-Rejection Algorithm** is a method used to sample from a complicated probability density function \( f(x) \) when direct sampling is difficult. It works by using a simpler **proposal distribution** \( g(x) \) and an acceptance criterion.

Given a target density function \( f(x) \) that we wish to sample from, the algorithm follows these steps:

1. **Choose a proposal distribution** \( g(x) \) such that:

   \[
   f(x) \leq M g(x), \quad \forall x
   \]

   where \( M \) is a constant chosen such that \( M g(x) \) always bounds \( f(x) \) from above.

2. **Generate candidate samples** \( x^* \sim g(x) \).

3. **Generate a uniform random number** \( u \sim U(0,1) \).

4. **Accept or reject the candidate** based on the probability condition:

   \[
   u \leq \frac{f(x^*)}{M g(x^*)}
   \]

   - If the condition holds, we accept \( x^* \).
   - Otherwise, we reject \( x^* \) and repeat the process until we obtain enough accepted samples.

---

### Proposal Distribution
We select a **normal distribution** as our proposal:

$$
g(x) = \mathcal{N}(0.4, 1.2^2)
$$

which means \( g(x) \) is a **Gaussian distribution** centered at \( 0.4 \) with a standard deviation of \( 1.2 \). This is chosen because:

- The peak of \( f(x) \) is around \( x = 0.4 \), making the proposal more efficient.
- A normal distribution provides a smooth and well-behaved density function for sampling.

---

### Computing the Scaling Constant \( M \)
The value of \( M \) is chosen such that:

\[
M = \max_x \frac{f(x)}{g(x)}
\]

Using numerical optimization, we determine:

\[
M = 76.1
\]

---

### Acceptance Rate
The probability of accepting a candidate sample is:

\[
P(\text{accept}) = \frac{1}{M}
\]

Substituting \( M = 76.1 \):

\[
P(\text{accept}) = \frac{1}{76.1} \approx 0.0131 \text{ (or 1.31%)}
\]

This implies that **only 1.31% of proposed samples** are accepted, making the process computationally inefficient.

---

### Algorithm Summary
The algorithm can be summarized as follows:

1. Generate \( x^* \sim g(x) \) (from \( \mathcal{N}(0.4, 1.2^2) \)).
2. Generate \( u \sim U(0,1) \).
3. If

   \[
   u \leq \frac{f(x^*)}{M g(x^*)}
   \]

   accept \( x^* \); otherwise, reject and repeat.

---

### Visualization
To understand the effectiveness of our sampling process, we compare:

1. The **accepted samples' histogram** to the actual target density \( f(x) \).
2. The **proposal distribution histogram** to visualize how well \( g(x) \) approximates \( f(x) \).

These comparisons help us evaluate whether our choice of \( g(x) \) is reasonable or if we need a better proposal distribution to improve efficiency.

---

### Efficiency Considerations
- **Low Acceptance Rate (1.31%)**: A large proportion of generated samples are rejected.
- **Improving Efficiency**:
  - A **better proposal distribution** that more closely matches \( f(x) \).
  - Using **adaptive rejection sampling** to dynamically adjust \( g(x) \).

This method, while simple, is **widely used in Monte Carlo simulations** and can be improved by tuning \( g(x) \) and \( M \).

---

```{r}
set.seed(123) 

f <- function(x) {
  exp(0.4 * (x - 0.4)^2 - 0.08 * x^4)
}

proposal_dist <- function(n) {
  rnorm(n, mean = 0.4, sd = 1.2)  # Centered near the peak of f(x)
}

# Define the corresponding density function g(x)
g <- function(x) {
  dnorm(x, mean = 0.4, sd = 1.2)
}

# Find the optimal value of M empirically
optimize_M <- function() {
  x_vals <- seq(-3, 3, length.out = 1000)
  ratio_vals <- f(x_vals) / g(x_vals)
  max(ratio_vals, na.rm = TRUE)  # Maximum ratio to get the scaling constant M
}

M <- optimize_M()

accrej <- function(fdens, gdens, M, x){
                    U = runif(length(x))
                    selected = x[U <= (fdens(x) / (M * gdens(x)))]
                    return(selected)
                    }
x_candidates <- proposal_dist(10000)
y = accrej(f, g, M, x_candidates)
```


```{r}
hist(y, probability = TRUE, breaks = 50, col = "lightblue", main = "Acceptance-Rejection Sampling",
     xlab = "x", ylab = "Density")
curve(f(x) / integrate(f, -Inf, Inf)$value, add = TRUE, col = "red", lwd = 2)

hist(rnorm(10000, mean = 0.4, sd = 1.2), probability = TRUE, breaks = 50, col = "lightblue", main = "Proposal Distribution",
     xlab = "x", ylab = "Density")
```
```{r}
# serial
start_time_ar_serial <- Sys.time()
x_candidates <- proposal_dist(10000)
y_serial <- accrej(f, g, M, x_candidates)
end_time_ar_serial <- Sys.time()
ar_time_serial <- as.numeric(difftime(end_time_ar_serial, start_time_ar_serial, units = "secs"))

# parallel
num_cores <- 8
task <- rep(10000 / num_cores, num_cores)
cl <- makeCluster(num_cores)
clusterExport(cl, varlist = c("accrej", "f", "g", "M", "proposal_dist"))

start_time_parallel_ar <- Sys.time()
results_parallel_ar <- parLapply(cl, task, function(task) accrej(f, g, M, proposal_dist(task)))
end_time_parallel_ar <- Sys.time()

stopCluster(cl)
parallel_time_ar <- as.numeric(difftime(end_time_parallel_ar, start_time_parallel_ar, units = "secs"))
cat("Parallel computing time for Acceptance-Rejection Sampling:", parallel_time_ar, "secs\n")
```


### Adaptive Rejection Sampling (ARS)

**Adaptive Rejection Sampling (ARS)** constructs piecewise linear bounds around the log of the PDF, refining these bounds after evaluating the density at new points. This adaptation often yields higher acceptance rates than basic rejection sampling for **log-concave or nearly log-concave distributions**. ARS presents a black-box technique for sampling from a **log-concave probability density function** \( g(x) \), with the envelope function and the squeezing function (which form upper and lower bounds to \( g(x) \)) converging to the density \( g(x) \) as sampling proceeds.

To sample \( n \) points independently from \( g(x) \) by rejection sampling, ARS defines an **envelope function** \( g_u(x) \), where \( g_u(x) \geq g(x) \) for all \( x \) in the domain \( D \) of \( g(x) \), and optionally defines a **squeezing function** \( g_l(x) \), where \( g_l(x) \leq g(x) \) for all \( x \) in \( D \). Then, the following sampling steps are performed until \( n \) points have been accepted:

1. Sample a value \( x^* \) from \( g_u(x) \), and sample a value \( w \) independently from the uniform \( (0, 1) \) distribution.  
2. If \( w \leq \frac{g_l(x)}{g_u(x)} \), then accept \( x^* \). Otherwise, evaluate \( g(x^*) \) and perform the following rejection test.
3. If \( w \leq \frac{g_l(x^*)}{g_u(x^*)} \), then accept \( x^* \); otherwise, reject \( x^* \).
4. Repeat until \( n \) points have been accepted.

To define an envelope function, ARS selects **initial support points** where the log-density function and its derivative are evaluated, then constructs the upper envelope using tangent lines at these points. Finally, ARS also constructs the lower squeezing function using secant lines between adjacent points.

ARS reduces the number of function evaluations needed by adapting the envelope dynamically and can sample from any univariate log-concave density without requiring knowledge of the mode. However, ARS is only useful if it is more efficient or convenient to sample from the envelope \( g_u(x) \) than from the density \( g(x) \) itself. In practice, finding a suitable \( g_u(x) \) can be difficult and often involves locating the supremum of \( g(x) \) in \( D \) by using a standard optimization technique.

### Target Function

```{r}
#new
log_f <- function(x) {
  return(log(exp(0.4 * (x - 0.4)^2 - 0.08 * x^4)))
}
f <- function(x) {
  exp(0.4 * (x - 0.4)^2 - 0.08 * x^4)
}

dlog_f <- function(x) {
  grad(log_f, x)
}
# Initialize the support point
initialize_support <- function(n_points = 10) {
x_points <- seq(-4,4, length.out = n_points)

  support <- x_points

  #support <- unique(c(0.001, support))  

  return(support)
}
```


### Upper hull function and lower hull function

```{r}
upper_hull <- function(x, support) {
  h_vals <- log_f(support)
  slopes <- dlog_f(support)
  
  idx <- max(which(support <= x))
  idx <- min(idx, length(support) - 1)  
  
  x1 <- support[idx]
  x2 <- support[idx + 1]
  h1 <- h_vals[idx]
  h2 <- h_vals[idx + 1]
  s1 <- slopes[idx]
  s2 <- slopes[idx + 1]
  
  h_x <- min(h1 + s1 * (x - x1), h2 + s2 * (x - x2))
  return(h_x)
}

lower_hull <- function(x, support) {
  n <- length(support)
  for (i in 1:(n-1)) {
    x1 <- support[i]
    x2 <- support[i+1]
    y1 <- log_f(x1)
    y2 <- log_f(x2)
    if (x1 <= x && x <= x2) {
      slope <- (y2 - y1) / (x2 - x1)
      return(exp(y1 + slope * (x - x1)))  
    }
  }
  return(0)
}
```

### Sampling function

```{r}
sample_from_upper_hull <- function(support) {
  h_vals <- log_f(support)   
  slopes <- dlog_f(support)  

  lambda <- slopes
  exp_areas <- rep(0, length(support) - 1)  

  for (i in 1:(length(support) - 1)) {
    x1 <- support[i]
    x2 <- support[i + 1]
    h1 <- h_vals[i]
    h2 <- h_vals[i + 1]
    b <- slopes[i]
    b <- ifelse(is.na(b), 0, b)   
    if (b == 0) {
      exp_areas[i] <- (x2 - x1) * exp(h1)  
    } else {
      exp_areas[i] <- (exp(h2) - exp(h1)) / b  
    }
    
    exp_areas[i] <- max(exp_areas[i], 1e-10)  
  }

  probs <- exp_areas / sum(exp_areas)

  idx <- sample(1:(length(support) - 1), size = 1, prob = probs)
  x1 <- support[idx]
  x2 <- support[idx + 1]
    x_proposed <- runif(1, x1, x2)

  return(x_proposed)
}

```

### ARS

```{r}
ars <- function(n_samples, n_support = 15) {
  support <- initialize_support(n_support)
  samples <- numeric(n_samples)
  total_iterations <- 0 
  total_accepted <- 0 
  start_time <- Sys.time()
  
  for (i in 1:n_samples) {
    repeat {
     x_proposed <- sample_from_upper_hull(support)
      u <- runif(1)
      u_x <- upper_hull(x_proposed, support)
      l_x <- lower_hull(x_proposed, support)
      total_iterations <- total_iterations + 1
      if (is.na(u_x) || is.na(l_x)) {
        next
      }
      if (log(u) <= l_x - u_x) {
        samples[i] <- x_proposed
        total_accepted <- total_accepted + 1
        break
      } else if (log(u) <= log_f(x_proposed) - u_x) {
        samples[i] <- x_proposed
        total_accepted <- total_accepted + 1
        break
      }
    }
    support <- sort(c(support, samples[i]))
  }
  end_time <- Sys.time()  
  execution_time <- as.numeric(difftime(end_time, start_time, units = "secs"))  
  acceptance_rate <- total_accepted / total_iterations 
  avg_iterations <- total_iterations / total_accepted
  return(list(
    samples = samples,
    acceptance_rate = acceptance_rate,
    execution_time = execution_time,
    avg_iterations = avg_iterations
  ))
}
```

### Generate Samples - Target

```{r}
set.seed(4693)
result <- ars(10000)
cat("Acceptance Rate:", result$acceptance_rate, "\n")
cat("Execution Time:", result$execution_time, "seconds\n")
cat("Average Number of Iterations per Sample:", result$avg_iterations, "\n")


hist_data <- hist(result$samples, probability = TRUE, breaks = 50, col = "lightblue", border = "black",
                  xlab = "Sampled Values", ylab = "Density", main = "Histogram of Sampled Data with True Density")
x_vals <- seq(-3, 3, length.out = 1000)  # Adjust the range as needed
norm_factor <- integrate(f, -Inf, Inf)$value
norm_f <- function(x) f(x) / norm_factor

# Overlay the normalized density curve
curve(norm_f(x), col = "red", lwd = 2, add = TRUE)

#curve(normalize_target_function(x), add = TRUE, col = "red", lwd = 2, xlim = range(result$samples), ylim = c(0, 1))
legend("topright", legend = c("Histogram of Samples", "True Density"),
       col = c("lightblue", "red"), lwd = c(2, 2), fill = c("lightblue", NA))
```

As the result shows, the acceptance rate is `r result$acceptance_rate`, the execution time is `r result$execution_time` seconds, with the average number of iterations per sample `r result$avg_iterations`.

The sample from ARS aligns well with the target distribution, meaning the ARS method yields highly accurate results. However, the sampling point has vacancies in the second peak, which is due to the inability of the envelope function to adapt to the shape of the target distribution in time in the region of the second peak, resulting in overstretching or compression of the envelope function. Even though all the samples are accepted, they may be accepted only in certain regions, resulting in missing samples in other regions.

### Parallel

```{r}
num_cores <- 8 

set.seed(4693)
task <- rep(10000 / num_cores, num_cores)

#start_time_serial <- Sys.time()
#results_serial <- lapply(tasks, function(task) ars(n_samples = task))  
#end_time_serial <- Sys.time()
#serial_time <- as.numeric(difftime(end_time_serial, start_time_serial, units = "secs"))

cl <- makeCluster(num_cores)  
clusterExport(cl, varlist = c("ars", "initialize_support", "sample_from_upper_hull", "upper_hull", "lower_hull", "log_f", "dlog_f", 'grad', 'f'))
start_time_parallel <- Sys.time()
results_parallel <- parLapply(cl, task, function(task) ars(n_samples = task))  
end_time_parallel <- Sys.time()

stopCluster(cl)  

parallel_time <- as.numeric(difftime(end_time_parallel, start_time_parallel, units = "secs"))
speedup_factor <- result$execution_time / parallel_time


cat("Serial computing time:", result$execution_time, "secs\n")
cat("Parallel computing time:", parallel_time, "secs\n")
cat("Speedup Factor:", speedup_factor, "\n")
```

The serial computing time is `r result$execution_time`, and the parallel computing time is `r parallel_time`. The speedup factor is  `r result$execution_time / parallel_time`, meaning that using parallel computing is helpful for making ARS more efficient.

When simulating the generated data using the ARS method, **the acceptance rate is always 1**. This may be due to the fact that the **envelope function is very close to the original function, resulting in all samples being accepted**. Also, due to the relatively **standard log-concave nature** of the target distribution, this results in a high Acceptance rate.

Next, a **non-log concave Beta distribution** is generated to verify the code for ARS is correct.

### Beta

```{r eval=FALSE}
#beta
log_f <- function(x) {
  return(log(dbeta(x,0.5,0.5)))
}

dlog_f <- function(x) {
  grad(log_f, x)
}
# Initialize the support point
initialize_support <- function(n_points = 10) {
  p_seq <- seq(0.01, 0.99, length.out = n_points)  
  p_seq <- p_seq^2
  q1 <- qbeta(p_seq, 0.5, 0.5)  

  support <- unique(q1)

  #support <- unique(c(0.001, support))  

  return(support)
}

```

### Generate Samples - Beta

```{r eval=FALSE}
set.seed(4693)
result_2 <- ars(10000)
cat("Acceptance Rate:", result_2$acceptance_rate, "\n")
cat("Execution Time:", result_2$execution_time, "seconds\n")
cat("Average Number of Iterations per Sample:", result_2$avg_iterations, "\n")

hist_data <- hist(result_2$samples, probability = TRUE, breaks = 50, col = "lightblue", border = "black",
                  xlab = "Sampled Values", ylab = "Density", main = "Histogram of Sampled Data with True Density")
curve(dbeta(x,0.5,0.5), add = TRUE, col = "red", lwd = 2)
legend("topright", legend = c("Histogram of Samples", "True Density"),
       col = c("lightblue", "red"), lwd = c(2, 2), fill = c("lightblue", NA))
```

### Parallel

```{r eval=FALSE}
num_cores <- 8

set.seed(4693)
task <- rep(10000 / num_cores, num_cores)

#start_time_serial <- Sys.time()
#results_serial <- lapply(tasks, function(task) ars(n_samples = task))  
#end_time_serial <- Sys.time()
#serial_time <- as.numeric(difftime(end_time_serial, start_time_serial, units = "secs"))

cl <- makeCluster(num_cores)  
clusterExport(cl, varlist = c("ars", "initialize_support", "sample_from_upper_hull", "upper_hull", "lower_hull", "log_f", "dlog_f", 'grad'))
start_time_parallel <- Sys.time()
results_parallel <- parLapply(cl, task, function(task) ars(n_samples = task))  
end_time_parallel <- Sys.time()

stopCluster(cl)  

parallel_time_2 <- as.numeric(difftime(end_time_parallel, start_time_parallel, units = "secs"))
speedup_factor_2 <- result_2$execution_time / parallel_time


cat("Serial computing time:", result_2$execution_time, "secs\n")
cat("Parallel computing time:", parallel_time_2, "secs\n")
cat("Speedup Factor:", speedup_factor_2, "\n")
```

For $ Beta(0.5, 0.5) $, the acceptance rate is much lower, and the execution time is shorter, with the average number of iterations per sample smaller. ARS has a decrease in acceptance rate in sampling non-log-concave function, but the sample still fits the target distribution well.

In generating for $ Beta(0.5, 0.5) $, the fact that the acceptance rate is no longer 1 indicates that there is no problem with the code's logic for calculating the acceptance rate. This suggests that ARS cannot efficiently generate data for a particular distribution when dealing with a function that is not log-concave.


### Slice Sampling

**Slice sampling**, introduced by Neal (2003), constructs a Markov chain that explores the target distribution by iteratively defining and sampling from a slice of the density function.

Given a target density \( f(x) \), slice sampling introduces an auxiliary variable \( y \) such that:

\[
S = \{x : y < f(x)\}
\]

where \( y \) is drawn uniformly from \( (0, f(x_0)) \), ensuring that \( S \) contains the support of the distribution. The sampling process involves:

1. **Evaluate the target function**: Given a target distribution \( f(x) \), evaluate \( f(x) \) at the current location \( x_t \).

2. **Draw a point uniformly**: Draw a point \( x_0 \) uniformly between \( f(x_t) \) and the x-axis.

3. **Create a starting point**: Create a horizontal length \( w \) uniformly placed around \( x_0 \).

4. **Stepping out**: From the starting point, move left and right at length \( w \) until the boundaries L and R are above the target function.

5. **Pick a point within the interval**: Choose a point uniformly from [L, R] and check if it is below the target function. If the point is below the function, accept it as a new sample and make this point the new starting point \( x_{t+1} \).

6. **Shrinkage**: If the point is not below the target function, shrink the interval and sample again from the updated range.

7. **Repeat**: repeat the shrinkage step until a valid sample is collected.

8. **New start**: make \( x_{t+1} \) the new starting point and go back to step 1. Repeat step 1-8 until reaching the desired sample size.


Slice sampling ensures that each sampled point is within the probability region. The stepping out the shrinkage method allows the sampling to be adaptive to the shape of \( f(x) \).

### Initialize Sampler
```{r}
# Set initial values
x0 <- 0  # Starting point
w <- 0.2726316   # Step size for stepping out
n_samples <- 10000  # Number of samples
```

### Implement slice sampling
```{r}
set.seed(4693)

slice_sample <- function(f, x0, w, n_samples) {
  samples <- numeric(n_samples)  # Storage for samples
  iteration_counts <- numeric(n_samples) #track number of iteration per sample
  total_iterations <-0
  
  x <- x0  # Initialize x
  start_time <- Sys.time()

  for (i in 1:n_samples) {
    iter_count <- 0 #track iteration for this sample
    
    # Step 1: Sample auxiliary variable y
    y <- runif(1, 0, f(x))
    
    # Step 2: Find an interval (L, R) around x using stepping out
    L <- x - w * runif(1)  # Randomly position initial interval
    R <- L + w
    
    # Expand L and R until they are outside the slice
    while (f(L) > y) { L <- L - w }
    while (f(R) > y) { R <- R + w }

    # Step 3: Sample a new x within (L, R) using shrinkage
    repeat {
      iter_count <- iter_count + 1
      x_new <- runif(1, L, R)  # Sample uniformly from (L, R)
      if (f(x_new) > y) {  # Accept if inside the slice
        x <- x_new
        break
      } else {  # Shrink the interval
        if (x_new < x) { L <- x_new } else { R <- x_new }
      }
    }
    
    # Store the new sample
    samples[i] <- x
    iteration_counts[i] <- iter_count
    total_iterations <- total_iterations + iter_count
  }
  end_time <- Sys.time()  # End timing
  computing_time <- end_time - start_time
  
  # Compute average number of iterations
  avg_iterations <- mean(iteration_counts)
  
  list(
    samples = samples,
    acceptance_rate = n_samples/total_iterations,
    avg_iterations_per_sample = avg_iterations,
    total_iterations = total_iterations,
    total_computing_time = computing_time
  )
}

```

```{r}
# Run the slice sampling algorithm
res <- slice_sample(f, x0, w, n_samples)

res$acceptance_rate
res$avg_iterations_per_sample
res$total_computing_time
```


```{r}
set.seed(4693)
hist(res$samples, probability = TRUE, breaks = 40, col = "lightblue",
     main = "Histogram of Samples (Slice Sampling)",
     xlab = "x", border = "black")

# Define the theoretical density function
f = function(x) {
  exp(0.4 * (x - 0.4)^2 - 0.08 * x^4)
}

# Normalize the density function
x_vals <- seq(-3, 3, length.out = 1000)  # Adjust the range as needed
norm_factor <- integrate(f, -Inf, Inf)$value
norm_f <- function(x) f(x) / norm_factor

# Overlay the normalized density curve
curve(norm_f(x), col = "red", lwd = 2, add = TRUE)

```

```{r}
# Define the range of w values
w_values <- seq(0.01, 5, length.out = 20)

# Initialize a data frame to store results
results <- data.frame(w = numeric(), 
                      acceptance_rate1 = numeric(), 
                      avg_iterations1 = numeric(), 
                      computing_time1 = numeric())

# Loop through w values
for (w in w_values) {
  start_time1 <- Sys.time()  # Start timing
  res1 <- slice_sample(f, x0, w, n_samples)
  end_time1 <- Sys.time()    # End timing
  
  # Store results
  results <- rbind(results, data.frame(
    w = w,
    acceptance_rate1 = res1$acceptance_rate,
    avg_iterations1 = res1$avg_iterations_per_sample,
    computing_time1 = as.numeric(difftime(end_time1, start_time1, units = "secs"))
  ))
}

# Plot acceptance rate vs. w
ggplot(results, aes(x = w, y = acceptance_rate1)) +
  geom_line() + geom_point() +
  labs(title = "Acceptance Rate vs. w", x = "w", y = "Acceptance Rate")

# Plot average iterations per sample vs. w
ggplot(results, aes(x = w, y = avg_iterations1)) +
  geom_line() + geom_point() +
  labs(title = "Average Iterations per Sample vs. w", x = "w", y = "Avg Iterations")

# Plot computing time vs. w
ggplot(results, aes(x = w, y = computing_time1)) +
  geom_line() + geom_point() +
  labs(title = "Computing Time vs. w", x = "w", y = "Computing Time (s)")

```

```{r}
results[1, ]
results[2, ]
```

### Parallel

```{r}
set.seed(4693)
num_cores = 8
task = rep(10000 / num_cores, num_cores)
cl <- makeCluster(num_cores)
clusterExport(cl, varlist = c("slice_sample", "f", "x0", "w", "n_samples"))
start_time_slice <- Sys.time()
slice_results <- parLapply(cl, task, function(task) slice_sample(f, x0, w, task))
end_time_slice <- Sys.time()
stopCluster(cl)
slice_time <- as.numeric(difftime(end_time_slice, start_time_slice, units = "secs"))

print(slice_time)
```


## Discussion

To evaluate the three sampling methods, we compare them based on the following criteria:

1. **Acceptance Rate**: The proportion of proposed samples that are accepted.

2. **Average Iterations Per Sample**: Measures efficiency in generating valid samples.

3. **Serial Computing Time**: The execution time required to generate 10,000 samples without 8 CPU core parallel computing.

4. **Parallel Computing Time**: The execution time required to generate 10,000 samples with 8 CPU core parallel computing.

5. **Accuracy**: How well the empirical distribution matches the true target distribution.

### Key Findings

```{r}
comparison_table <- data.frame(
  Method = c("AR", "ARS", "Slice Sampling"),
  "Acceptance Rate (%)" = c(100 / M, 100, res$acceptance_rate * 100),
  "Avg. Iterations" = c(M, 1.0, res$avg_iterations_per_sample),
  "Serial Computing Time (s)" = c(ar_time_serial, result$execution_time, res$total_computing_time),
  "Parallel Computing Time (s)" = c(parallel_time_ar, parallel_time, slice_time),
  Accuracy = c("Moderate", "High", "High"),
  check.names = FALSE
) %>%
  mutate(
    `Acceptance Rate (%)` = round(`Acceptance Rate (%)`, 1),
    `Avg. Iterations` = round(`Avg. Iterations`, 1),
    `Serial Computing Time (s)` = round(`Serial Computing Time (s)`, 4),
    `Parallel Computing Time (s)` = round(`Parallel Computing Time (s)`, 4)
  )

knitr::kable(comparison_table, caption = "Comparison of Sampling Methods")
```

- ARS has the highest acceptance rate but may not work well for non-log-concave distributions.

- Slice Sampling is the most flexible, working well for multimodal and non-log-concave distributions.

- Basic Acceptance-Rejection Algorithm is inefficient unless the proposal distribution is well-matched.

### Performance Comparison

From our results, we observe some key performance differences among the three sampling methods:

**Basic Acceptance-Rejection Sampling (AR)** exhibits a very low acceptance rate of 1.3%, requiring an average of `r M` iterations per sample. However, its computational time is very short, `r ar_time_serial`s in serial execution and `r parallel_time_ar`s in parallel execution. In summary, AR is not suitable for high-dimensional or multimodal distributions but useful for simple distributions due to its low computing cost with an efficient proposal function.

**Adaptive Rejection Sampling (ARS)** has the highest acceptance rate (100%) and the lowest average iterations per sample (1.0) among three methods, which suggests it generates samples efficiently per step. However, it requires significantly more computational time (`r result$execution_time`s in serial, `r parallel_time`s in parallel), primarily due to the overhead in constructing and updating the envelope function. ARS is a good choice for log-concave distributions, but it isn't applicable for non-log-concave distributions due to a significantly reduced acceptance rate and increased computing cost.

**Slice Sampling** has an acceptance rate of 90.6% and `r res$avg_iterations_per_sample` iterations per sample on average, making it highly efficient for our target distributions. Additionally, the computational time (`r res$total_computing_time`s in serial, `r slice_time`s in parallel) is significantly lower than ARS. Slice Sampling is particularly useful when dealing with complex target distributions that are difficult to approximate with a proposal distribution.

### Impact of Parallel Computing

Parallel computing significantly reduces the computational time for all three sampling methods:

AR: from `r ar_time_serial`s to `r parallel_time_ar`s (`r 100 * (ar_time_serial - parallel_time_ar) / ar_time_serial`% reduction, but negligible in absolute time). Although AR has extremely low acceptance rate, its single-time calculation cost is very low. 

ARS: from `r result$execution_time` s to `r parallel_time` s (`r 100 * (result$execution_time - parallel_time) / result$execution_time`% reduction). ARS needs to construct a convex hull upper bound function and dynamically update support points. Each new sample is added, envelope will be updated, resulting in higher computational costs.

Slice Sampling: from `r res$total_computing_time`s to `r slice_time`s (73.7% reduction). Slice Sampling requires determining the slice interval, and then iterating the step length within the slice interval, resulting in higher computational costs.

### Practical Implications

Efficiency balances accuracy, acceptance rate, and computational time. When selecting sampling methods in practical conditions:

If the target distribution is log-concave, ARS is a good choice due to its relatively high efficiency, with other trade-offs in computation time.

If the target distribution is non-log-concave, Slice Sampling is preferable as it maintains low computing cost and high accuracy.

If a well-matched proposal distribution can be chosen, AR can be effective because it have a low computing cost.

Parallel computing provides speedup for all three methods, and the greatest speedup for ARS. It is a good choice for large-scale simulations.



